---
title: "Prediction model on how well dumbbell lifts are performed"
author: "Nicholas Tang"
date: "Sunday, November 23, 2014"
output: html_document
keep_md: yes
---

#Executive Summary

The goal of our project is to produce a good machine learning model that predicts the *classe* variable, or the manner which a person does dumbbell lifts.

```{r, echo=FALSE, cache=TRUE, message=F, warning=F}
setInternet2(use = TRUE)
source("https://raw.githubusercontent.com/nicholas-yjtang/rutils/master/get_data.R")
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
get_data( "pml-training.csv",url)
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
get_data( "pml-testing.csv",url)
```

#Data
The source of the data is located at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  

The data consists of many features that come from sensors located on the body (arm, forearm, belt) and the dumbbell itself.  

```{r}
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!"))
```

#Data cleaning, Feature Selection

Exploring the data, we notice a few particular features which are not very useful  
1. The user's name  
2. The timestamp. It is mostly unique, so there is little value on its own  
3. The row number  

We will exclude these columns from our training

Apart from these, there are a number of columns which have many NA results (when window=FALSE). These columns are a summarised statistic when window=TRUE. Although this information can be important under certain scenarios, this information is not particular useful as it can only help prediction for these small subset of data, and the test set (as given) does not include any such information. As such we exclude both the window, and these columns with many NA results

```{r}
has_useful_information <- function(colname, data) {
  totalrecords <- nrow(training)
  returnnames <- ""
  if (sum(is.na(training[,colname]))/nrow(training)<0.05) {
    returnnames <- colname
  }
  returnnames
}
useful_variables <- sapply(names(training), has_useful_information, training)
useful_variables <- useful_variables[useful_variables!=""]
useful_variables <- useful_variables[!grepl("timestamp",useful_variables)]
useful_variables <- useful_variables[!grepl("^X$",useful_variables)]
useful_variables <- useful_variables[!grepl("window",useful_variables)]
useful_variables <- useful_variables[!grepl("user_name",useful_variables)]
final_training <- training[,useful_variables]

```

#Training and validation sets

We will partition our training data (randomly), into 60% for training, and 40% for validation. The training data will be used to build the model, and the validaton data is for the final out of sample error rate.  

```{r, message=F, warning=F}
library(caret)
set.seed(12345)
inTrain <- createDataPartition(y=final_training$classe,p=0.6, list=FALSE)
partTraining <- final_training[inTrain,]
partValidation <- final_training[-inTrain,]
```

#Model building

For our model, we will use boosted tree (gbm).  

As part of the model building, we will use k-fold cross validation(size of 10) with **trainControl**.  

```{r, message=F, warning=F, echo=FALSE} 
library(doSNOW)
cluster <- makeCluster(4) # should be the number of cores of your machine
registerDoSNOW(cluster)

```

```{r, message=F, warning=F, cache=TRUE}
fitControl <- trainControl(method="cv", number=10)
modFit <- train(classe ~ ., data=partTraining, method="gbm", trControl = fitControl, verbose=FALSE)

```

```{r,message=F, warning=F, echo=FALSE} 
stopCluster(cluster)
```

We look at the resampling results based on accuracy

```{r}
plot(modFit)
```

We have a look at the confusion matrix of this particular model (for in sample error rate)

```{r, message=F, warning=F, results='asis'}
library(pander)
prediction <- predict(modFit, partTraining)
prediction_results <- confusionMatrix(prediction, partTraining$classe)
pander(prediction_results$table)
```

The in sample accuracy of our model is **`r prediction_results$overall["Accuracy"]`**

#Out of sample expected error rate

Using our reserved validation set (40% previously partitioned), we will use this set to calculate the expected out of sample error rate. First the confusion matrix

```{r, message=F, warning=F, results='asis'}
prediction <- predict(modFit, partValidation)
prediction_results <- confusionMatrix(prediction, partValidation$classe)
pander(prediction_results$table)
```

The out of sample accuracy of our model is **`r prediction_results$overall["Accuracy"]`**

We notice the rate is relatively close to the calculated in sample accuracy rate

```{r, echo=F, message=F, warning=F,results='hide'}
test <- read.csv("pml-testing.csv")
just_variables <- useful_variables[!grepl("classe",useful_variables)]
final_test <- test[,just_variables]
prediction <- predict(modFit, final_test)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(prediction)

```